{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test task for the position of Data Engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment includes 3 small tasks. It is **recommended** to leave comments in each task, code should be formatted according to **PEP8**. Tasks should be performed without using Pandas and yandex-weather-api.\n",
    "\n",
    "**Before performing a test task, you should copy the notebook to your disk, and perform the test task in your copy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1. Downloading data from Yandex.Weather API and converting it to csv\n",
    "\n",
    "Using Yandex.Weather API, it is necessary to upload forecast data for 7 days for Moscow, Kazan, St. Petersburg, Tula and Novosibirsk. In case the API gives empty values for a day, they should be removed.\n",
    "\n",
    "The information should be presented by hours with an extended set of fields for precipitation.\n",
    "\n",
    "The received json should be converted to csv format:\n",
    "\n",
    "\\{begin{array}{ccc}\n",
    "\\text{city}, \\text{date}, \\text{hour}, \\text{temperature_c}, \\text{pressure_mm}, \\text{is_rainy} \\\\\n",
    "Moscow, 08/19/2023, 12, 27, 750, 0\\.\n",
    "Moscow, 08/19/2023, 13, 27, 750, 0\\.\n",
    "... \\\\\n",
    "Kazan, 19.08.2023, 12, 20, 770, 1\\\\ ...\n",
    "Kazan, 08/19/2023, 13, 21, 770, 0 \\.\n",
    "\\end{array}\n",
    "\n",
    "**Field Description:**\n",
    "\n",
    "city - City\n",
    "\n",
    "date - Date of event\n",
    "\n",
    "hour - Hours\n",
    "\n",
    "temperature_c - Temperature in Celsius\n",
    "\n",
    "pressure_mm - Pressure in mmHg\n",
    "\n",
    "is_rainy - Flag of rain in a particular day and hour (see API documentation - field description).\n",
    "\n",
    "The resulting csv should be uploaded to a cloud disk and a link should be provided at the end of the solution.\n",
    "\n",
    "**Link to get the key:** https://yandex.ru/dev/weather/doc/dg/concepts/about.html#about__onboarding\n",
    "\n",
    "\n",
    "**Additional questions:** What are the possible ways to speed up API data retrieval and conversion? Is it possible to use these ways in Airflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "cities_coordinates = {\n",
    "    \"Moscow\": {\"latitude\": 55.7558, \"longitude\": 37.6176},\n",
    "    \"Kazan\": {\"latitude\": 55.8304, \"longitude\": 49.0661},\n",
    "    \"St. Petersburg\": {\"latitude\": 59.9343, \"longitude\": 30.3351},\n",
    "    \"Tula\": {\"latitude\": 54.2021, \"longitude\": 37.6177},\n",
    "    \"Novosibirsk\": {\"latitude\": 55.0084, \"longitude\": 82.9357}\n",
    "}\n",
    "\n",
    "url = \"https://api.weather.yandex.ru/v2/forecast?\"\n",
    "language = \"en_US\"\n",
    "api_key = \"a20bd4f4-9675-47b2-bd24-9710abbf49de\"\n",
    "\n",
    "headers = {\n",
    "    \"X-Yandex-API-Key\": api_key\n",
    "}\n",
    "\n",
    "weather_results = {}\n",
    "\n",
    "for city, coordinates in cities_coordinates.items():\n",
    "    params = {\n",
    "        \"lat\": coordinates[\"latitude\"],\n",
    "        \"lon\": coordinates[\"longitude\"],\n",
    "        \"lang\": language,\n",
    "        \"limit\": 7,\n",
    "        \"hours\": 'true',\n",
    "        \"extra\": 'true'\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "        weather_results[city] = weather_data\n",
    "    else:\n",
    "        print(f\"Error getting weather data for {city}: {response.status_code}\")\n",
    "\n",
    "\n",
    "weather_results_json = json.dumps(weather_results, ensure_ascii=False, indent=4)\n",
    "#print(weather_results_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created.\n"
     ]
    }
   ],
   "source": [
    "weather_results = json.loads(weather_results_json)\n",
    "import csv\n",
    "# Create a CSV file\n",
    "with open('weather_data.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['city','date','hour','temperature_c','pressure_mm','is_rainy']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Record weather data for each city in CSV\n",
    "    for city, city_data in weather_results.items():\n",
    "        if 'forecasts' in city_data:\n",
    "\n",
    "            for forecast in city_data['forecasts']:\n",
    "                date = forecast['date']\n",
    "\n",
    "                for hour_data in forecast['hours']:\n",
    "                    hour = hour_data['hour']\n",
    "                    temperature_c = hour_data['temp']\n",
    "                    pressure_mm = hour_data['pressure_mm']\n",
    "                    is_rainy = hour_data['prec_type']\n",
    "\n",
    "                    # Creating CSV\n",
    "                    writer.writerow({\n",
    "                        'city': city,\n",
    "                        'date': date,\n",
    "                        'hour': hour,\n",
    "                        'temperature_c': temperature_c,\n",
    "                        'pressure_mm': pressure_mm,\n",
    "                        'is_rainy': is_rainy\n",
    "                    })\n",
    "\n",
    "print(\"CSV created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Loading data into the database (PostgreSQL).\n",
    "\n",
    "Using the received csv file, it is necessary to load data into PostgreSQL. Beforehand, it is necessary to create schemas in the database: for receiving raw data and for future aggregation tables.\n",
    "\n",
    "When creating tables, the use of partitioning and indexing (if possible and necessary) is encouraged.\n",
    "\n",
    "The solution needs to show the data loading code, scripts for creating schemas and tables for item 2 and 2.1.\n",
    "\n",
    "Hint: to solve the problem you need to deploy the database, we recommend to do it locally using docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run --name kate_postgres -e POSTGRES_PASSWORD=password -d -p 5432:5432 postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker ps # check if docker is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psql -h localhost -U postgres -d postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE SCHEMA IF NOT EXISTS raw_data;\n",
    "CREATE SCHEMA IF NOT EXISTS aggregation;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE IF NOT EXISTS raw_data.weather (\n",
    "    id SERIAL,\n",
    "    city VARCHAR(255),\n",
    "    date DATE,\n",
    "    hour INT8,\n",
    "    temperature_c FLOAT,\n",
    "    pressure_mm FLOAT,\n",
    "    is_rainy INT2\n",
    ") PARTITION BY LIST(city);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE raw_data.weather_data_moscow PARTITION OF raw_data.weather\n",
    "    FOR VALUES IN ('Moscow');\n",
    "\n",
    "CREATE TABLE raw_data.weather_data_kazan PARTITION OF raw_data.weather\n",
    "    FOR VALUES IN ('Kazan');\n",
    "\n",
    "CREATE TABLE raw_data.weather_data_petersburg PARTITION OF raw_data.weather\n",
    "    FOR VALUES IN ('St. Petersburg');\n",
    "\n",
    "CREATE TABLE raw_data.weather_data_tula PARTITION OF raw_data.weather\n",
    "    FOR VALUES IN ('Tula');\n",
    "\n",
    "CREATE TABLE raw_data.weather_data_novosibirsk PARTITION OF raw_data.weather\n",
    "    FOR VALUES IN ('Novosibirsk');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE IF NOT EXISTS aggregation.daily_weather (\n",
    "    city VARCHAR(255),\n",
    "    date DATE,\n",
    "    min_temperature_c FLOAT,\n",
    "    max_temperature_c FLOAT,\n",
    "    avg_temperature_c FLOAT,\n",
    "    min_pressure_mm FLOAT,\n",
    "    max_pressure_mm FLOAT,\n",
    "    avg_pressure_mm FLOAT,\n",
    "    rainy_days INTEGER,\n",
    "    PRIMARY KEY (city, date, hour)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE INDEX IF NOT EXISTS idx_aggregated_weather_city_date ON aggregation.daily_weather (city, date);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to PostgreSQL database successful.\n",
      "Data successfully loaded into PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# Establish connection to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    "    )\n",
    "    print(\"Connection to PostgreSQL database successful.\")\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Load data from CSV into PostgreSQL\n",
    "    with open('weather_data.csv', 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            cur.execute(\n",
    "                \"INSERT INTO raw_data.weather (city, date, hour, temperature_c, pressure_mm, is_rainy) VALUES (%s, %s, %s, %s, %s, %s)\",\n",
    "                (row['city'], row['date'], row['hour'], row['temperature_c'], row['pressure_mm'], row['is_rainy'])\n",
    "            )\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Close cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"Data successfully loaded into PostgreSQL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to PostgreSQL database:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Forming a View (PostgreSQL).\n",
    "\n",
    "1. Using a table with raw data, we need to build a View where for each city and day the rain start hours will be specified. Let's condition that rain can start only 1 time in a day in any of the cities.\n",
    "\n",
    "2. It is necessary to create a View where for each city, day and hour a moving average of temperature and pressure will be calculated.\n",
    "\n",
    "\n",
    "The resulting queries need to be inserted into google colab, and the results need to be uploaded in csv/xlsx format and posted as a link in google colab.\n",
    "\n",
    "Hint: if the raw file did not contain the fact of the beginning of rain, then it is necessary to randomize the values of the fact of rain in the table with raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CREATE OR REPLACE VIEW aggregation.rain_start_hours AS\n",
    "SELECT \n",
    "    city,\n",
    "    date,\n",
    "    MIN(hour) AS rain_start_hour\n",
    "FROM \n",
    "    raw_data.weather\n",
    "WHERE \n",
    "    is_rainy > 0 \n",
    "GROUP BY \n",
    "    city, date;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE VIEW aggregation.rolling_avg_weather AS\n",
    "SELECT\n",
    "    city,\n",
    "    date,\n",
    "    hour,\n",
    "    AVG(temperature_c) OVER (PARTITION BY city, date ORDER BY hour ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg_temperature_c,\n",
    "    AVG(pressure_mm) OVER (PARTITION BY city, date ORDER BY hour ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg_pressure_mm\n",
    "FROM\n",
    "    raw_data.weather;\n",
    "\n",
    "-- \n",
    "CREATE OR REPLACE VIEW aggregation.weather_vitrine AS\n",
    "SELECT\n",
    "    r.city,\n",
    "    r.date,\n",
    "    r.hour,\n",
    "    r.rolling_avg_temperature_c,\n",
    "    r.rolling_avg_pressure_mm,\n",
    "    CASE \n",
    "        WHEN r.hour = rv.rain_start_hour THEN true \n",
    "        ELSE false \n",
    "    END AS is_rain_start\n",
    "FROM\n",
    "    aggregation.rolling_avg_weather r\n",
    "LEFT JOIN\n",
    "    aggregation.rain_start_hours rv ON r.city = rv.city AND r.date = rv.date;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    "    )\n",
    "\n",
    "with open('weather_vitrine.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT * FROM aggregation.weather_vitrine\")\n",
    "\n",
    "        csvwriter.writerow([desc[0] for desc in cursor.description])\n",
    "\n",
    "        csvwriter.writerows(cursor)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. Task of designing a database based on Yandex.Metrics data\n",
    "\n",
    "The functionality of Yandex.Metrics includes the ability to pump out raw data using API: views and visits are pumped out by separate requests. For this process it is necessary to design a database, providing for several data layers and \"wants\" of customers: in 90% of cases customers need data aggregates (for example, to build a funnel by visits on pages and phone number input in terms of dates, pages, utm tags, or to build user flow by device, OS, etc.).\n",
    "\n",
    "The result should be provided in the form of a schema with description.\n",
    "\n",
    "Links to table structure:\n",
    "\n",
    "https://yandex.ru/dev/metrika/doc/api2/logs/fields/hits.html\n",
    "\n",
    "https://yandex.ru/dev/metrika/doc/api2/logs/fields/visits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

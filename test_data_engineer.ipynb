{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test task for the position of Data Engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment includes 3 small tasks. It is **recommended** to leave comments in each task, code should be formatted according to **PEP8**. Tasks should be performed without using Pandas and yandex-weather-api.\n",
    "\n",
    "**Before performing a test task, you should copy the notebook to your disk, and perform the test task in your copy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1. Downloading data from Yandex.Weather API and converting it to csv\n",
    "\n",
    "Using Yandex.Weather API, it is necessary to upload forecast data for 7 days for Moscow, Kazan, St. Petersburg, Tula and Novosibirsk. In case the API gives empty values for a day, they should be removed.\n",
    "\n",
    "The information should be presented by hours with an extended set of fields for precipitation.\n",
    "\n",
    "The received json should be converted to csv format:\n",
    "\n",
    "\\{begin{array}{ccc}\n",
    "\\text{city}, \\text{date}, \\text{hour}, \\text{temperature_c}, \\text{pressure_mm}, \\text{is_rainy} \\\\\n",
    "Moscow, 08/19/2023, 12, 27, 750, 0\\.\n",
    "Moscow, 08/19/2023, 13, 27, 750, 0\\.\n",
    "... \\\\\n",
    "Kazan, 19.08.2023, 12, 20, 770, 1\\\\ ...\n",
    "Kazan, 08/19/2023, 13, 21, 770, 0 \\.\n",
    "\\end{array}\n",
    "\n",
    "**Field Description:**\n",
    "\n",
    "city - City\n",
    "\n",
    "date - Date of event\n",
    "\n",
    "hour - Hours\n",
    "\n",
    "temperature_c - Temperature in Celsius\n",
    "\n",
    "pressure_mm - Pressure in mmHg\n",
    "\n",
    "is_rainy - Flag of rain in a particular day and hour (see API documentation - field description).\n",
    "\n",
    "The resulting csv should be uploaded to a cloud disk and a link should be provided at the end of the solution.\n",
    "\n",
    "**Link to get the key:** https://yandex.ru/dev/weather/doc/dg/concepts/about.html#about__onboarding\n",
    "\n",
    "\n",
    "**Additional questions:** What are the possible ways to speed up API data retrieval and conversion? Is it possible to use these ways in Airflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "cities_coordinates = {\n",
    "    \"Moscow\": {\"latitude\": 55.7558, \"longitude\": 37.6176},\n",
    "    \"Kazan\": {\"latitude\": 55.8304, \"longitude\": 49.0661},\n",
    "    \"St. Petersburg\": {\"latitude\": 59.9343, \"longitude\": 30.3351},\n",
    "    \"Tula\": {\"latitude\": 54.2021, \"longitude\": 37.6177},\n",
    "    \"Novosibirsk\": {\"latitude\": 55.0084, \"longitude\": 82.9357}\n",
    "}\n",
    "\n",
    "url = \"https://api.weather.yandex.ru/v2/forecast?\"\n",
    "language = \"en_US\"\n",
    "api_key = \"a20bd4f4-9675-47b2-bd24-9710abbf49de\"\n",
    "\n",
    "headers = {\n",
    "    \"X-Yandex-API-Key\": api_key\n",
    "}\n",
    "\n",
    "weather_results = {}  \n",
    "\n",
    "for city, coordinates in cities_coordinates.items():\n",
    "    params = {\n",
    "        \"lat\": coordinates[\"latitude\"],\n",
    "        \"lon\": coordinates[\"longitude\"],\n",
    "        \"lang\": language,\n",
    "        \"limit\": 7,\n",
    "        \"hours\": 'true',\n",
    "        \"extra\": 'true'\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "        weather_results[city] = weather_data\n",
    "    else:\n",
    "        print(f\"Error getting weather data for {city}: {response.status_code}\")\n",
    "\n",
    "\n",
    "weather_results_json = json.dumps(weather_results, ensure_ascii=False, indent=4)\n",
    "#print(weather_results_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created.\n"
     ]
    }
   ],
   "source": [
    "weather_results = json.loads(weather_results_json)\n",
    "import csv\n",
    "# Create a CSV file \n",
    "with open('weather_data.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['city','date','hour','temperature_c','pressure_mm','is_rainy']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Record weather data for each city in CSV\n",
    "    for city, city_data in weather_results.items():\n",
    "        if 'forecasts' in city_data:\n",
    "            \n",
    "            for forecast in city_data['forecasts']:\n",
    "                date = forecast['date']\n",
    "                \n",
    "                for hour_data in forecast['hours']:\n",
    "                    hour = hour_data['hour']\n",
    "                    temperature_c = hour_data['temp']\n",
    "                    pressure_mm = hour_data['pressure_mm']\n",
    "                    is_rainy = hour_data['prec_type']\n",
    "                    \n",
    "                    # Creating CSV\n",
    "                    writer.writerow({\n",
    "                        'city': city,\n",
    "                        'date': date,\n",
    "                        'hour': hour,\n",
    "                        'temperature_c': temperature_c,\n",
    "                        'pressure_mm': pressure_mm,\n",
    "                        'is_rainy': is_rainy\n",
    "                    })\n",
    "\n",
    "print(\"CSV created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Loading data into the database (PostgreSQL).\n",
    "\n",
    "Using the received csv file, it is necessary to load data into PostgreSQL. Beforehand, it is necessary to create schemas in the database: for receiving raw data and for future aggregation tables.\n",
    "\n",
    "When creating tables, the use of partitioning and indexing (if possible and necessary) is encouraged.\n",
    "\n",
    "The solution needs to show the data loading code, scripts for creating schemas and tables for item 2 and 2.1.\n",
    "\n",
    "Hint: to solve the problem you need to deploy the database, we recommend to do it locally using docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run --name my_postgres -e POSTGRES_PASSWORD=password -d -p 5432:5432 postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker ps # check if docker is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to PostgreSQL database successful.\n",
      "Data successfully loaded into PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# Establish connection to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    "    )\n",
    "    print(\"Connection to PostgreSQL database successful.\")\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Load data from CSV into PostgreSQL\n",
    "    with open('weather_data.csv', 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        # for row in reader:\n",
    "        #     cur.execute(\n",
    "        #         \"INSERT INTO raw_data.weather (city, date, hour, temperature_c, pressure_mm, is_rainy) VALUES (%s, %s, %s, %s, %s, %s)\",\n",
    "        #         (row['city'], row['date'], row['hour'], row['temperature_c'], row['pressure_mm'], row['is_rainy'])\n",
    "        #     )\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Close cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"Data successfully loaded into PostgreSQL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to PostgreSQL database:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Forming a View (PostgreSQL).\n",
    "\n",
    "1. Using a table with raw data, we need to build a View where for each city and day the rain start hours will be specified. Let's condition that rain can start only 1 time in a day in any of the cities.\n",
    "\n",
    "2. It is necessary to create a View where for each city, day and hour a moving average of temperature and pressure will be calculated.\n",
    "\n",
    "\n",
    "The resulting queries need to be inserted into google colab, and the results need to be uploaded in csv/xlsx format and posted as a link in google colab.\n",
    "\n",
    "Hint: if the raw file did not contain the fact of the beginning of rain, then it is necessary to randomize the values of the fact of rain in the table with raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. Task of designing a database based on Yandex.Metrics data\n",
    "\n",
    "The functionality of Yandex.Metrics includes the ability to pump out raw data using API: views and visits are pumped out by separate requests. For this process it is necessary to design a database, providing for several data layers and \"wants\" of customers: in 90% of cases customers need data aggregates (for example, to build a funnel by visits on pages and phone number input in terms of dates, pages, utm tags, or to build user flow by device, OS, etc.).\n",
    "\n",
    "The result should be provided in the form of a schema with description.\n",
    "\n",
    "Links to table structure:\n",
    "\n",
    "https://yandex.ru/dev/metrika/doc/api2/logs/fields/hits.html\n",
    "\n",
    "https://yandex.ru/dev/metrika/doc/api2/logs/fields/visits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
